{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习算法的实现\n",
    "\n",
    "## 前提\n",
    "\n",
    "神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。\n",
    "\n",
    "### Step1 - mini-batch\n",
    "\n",
    "从训练集中随机选择一小部分数据，称为“mini-batch”，其目标是减小mini-batch的损失函数值。\n",
    "\n",
    "### Step2 - 计算梯度\n",
    "\n",
    "为了减小mini-batch的损失函数值，需要求出各个权重和偏置的梯度。梯度表示损失函数值减小最多的方向。\n",
    "\n",
    "### Step3 - 更新权重和偏置\n",
    "\n",
    "根据梯度，调整权重和偏置的值，进行微小调整，使损失函数值减小。\n",
    "\n",
    "### Step4 - 重复\n",
    "\n",
    "重复上述步骤，直到损失函数值不再减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini-batch数据，所以又称为**随机梯度下降法（stochastic gradient descent）**。\n",
    ">“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。SGD来源于随机梯度下降法的英文名称的首字母。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写数字识别的神经网络实现（2层）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个两层神经网络TwoLayerNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # 初始化权重和偏置, 参数依次为输入层节点数、隐藏层节点数和输出层节点数。\n",
    "    # weight_init_std表示权重的初始化标准差。\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        # params是保存权重和偏置的字典，params['W1']表示第1层权重矩阵，params['b1']表示第1层偏置向量。\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 计算前向传播结果，x是输入数据。\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        # 前向传播\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # 计算损失函数，x和t是输入和标签。\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 计算准确率， x和t是输入和标签。\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 计算模型参数的梯度，x和t是输入和标签。\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        # 保存梯度的字典，grads['W1']表示第1层权重矩阵的梯度，grads['b1']表示第1层偏置向量的梯度。\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    # 反向传播\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1  # 线性变换\n",
    "        z1 = sigmoid(a1) # 激活函数\n",
    "        \n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10, 784) # 模拟10个手写数字数据\n",
    "t = np.random.rand(10, 10) # 模拟标签\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 计算梯度\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用mini-batch方法对MNIST数据集进行训练，采用网络维护TwoLayerNet类，并采用SGD算法进行训练。\n",
    "\n",
    "进行学习的过程中，会定期地对训练数据和测试数据记录识别精度。这里，每经过一个epoch，都会记录下训练数据和测试数据的识别精度。\n",
    "\n",
    "> epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于10000笔训练数据，用大小为100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被“看过”了。此时，100次就是一个epoch。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n",
      "iter_per_epoch: 600.0\n",
      "epoch 0, loss: 229.76062919984824, train acc: 0.09443333333333333, test acc: 0.0952\n",
      "epoch 600, loss: 85.82234074625254, train acc: 0.78595, test acc: 0.7914\n",
      "epoch 1200, loss: 67.19452982353754, train acc: 0.87545, test acc: 0.879\n",
      "epoch 1800, loss: 41.27280231247918, train acc: 0.8972666666666667, test acc: 0.8999\n",
      "epoch 2400, loss: 25.795691173876637, train acc: 0.9065666666666666, test acc: 0.9097\n",
      "epoch 3000, loss: 25.418646213775926, train acc: 0.9131166666666667, test acc: 0.9166\n",
      "epoch 3600, loss: 22.974707214406674, train acc: 0.9196333333333333, test acc: 0.9208\n",
      "epoch 4200, loss: 34.77906941222805, train acc: 0.9232166666666667, test acc: 0.925\n",
      "epoch 4800, loss: 19.798331883639463, train acc: 0.9274166666666667, test acc: 0.928\n",
      "epoch 5400, loss: 31.79673127720406, train acc: 0.9302666666666667, test acc: 0.9327\n",
      "epoch 6000, loss: 37.30948550979641, train acc: 0.9335333333333333, test acc: 0.9342\n",
      "epoch 6600, loss: 27.271564989088684, train acc: 0.93655, test acc: 0.936\n",
      "epoch 7200, loss: 23.4864174155655, train acc: 0.9391666666666667, test acc: 0.9392\n",
      "epoch 7800, loss: 9.452274331412921, train acc: 0.9418166666666666, test acc: 0.941\n",
      "epoch 8400, loss: 21.582117780411313, train acc: 0.9435166666666667, test acc: 0.9426\n",
      "epoch 9000, loss: 20.026958625083363, train acc: 0.9446666666666667, test acc: 0.9433\n",
      "epoch 9600, loss: 11.915877661445377, train acc: 0.9471166666666667, test acc: 0.946\n",
      "epoch 10200, loss: 27.873120717385817, train acc: 0.9486, test acc: 0.9475\n",
      "epoch 10800, loss: 19.449484055773176, train acc: 0.9502, test acc: 0.9484\n",
      "epoch 11400, loss: 15.674941830302114, train acc: 0.9519166666666666, test acc: 0.9507\n",
      "epoch 12000, loss: 6.337610299903055, train acc: 0.9530333333333333, test acc: 0.9513\n",
      "epoch 12600, loss: 16.98691613877964, train acc: 0.9545333333333333, test acc: 0.9515\n",
      "epoch 13200, loss: 21.63665676810087, train acc: 0.9556166666666667, test acc: 0.9526\n",
      "epoch 13800, loss: 11.03693055619572, train acc: 0.9565, test acc: 0.9541\n",
      "epoch 14400, loss: 20.695756322098447, train acc: 0.9574, test acc: 0.9542\n",
      "epoch 15000, loss: 21.036152709505807, train acc: 0.9580666666666666, test acc: 0.9551\n",
      "epoch 15600, loss: 18.423625860815164, train acc: 0.9593166666666667, test acc: 0.9553\n",
      "epoch 16200, loss: 9.59705143956415, train acc: 0.9601, test acc: 0.957\n",
      "epoch 16800, loss: 12.457603913113754, train acc: 0.9612, test acc: 0.9576\n",
      "epoch 17400, loss: 9.501536302587741, train acc: 0.9619833333333333, test acc: 0.9573\n",
      "epoch 18000, loss: 8.66289366325637, train acc: 0.963, test acc: 0.9581\n",
      "epoch 18600, loss: 16.731157772787856, train acc: 0.9629, test acc: 0.958\n",
      "epoch 19200, loss: 8.395589530521404, train acc: 0.9643166666666667, test acc: 0.9604\n",
      "epoch 19800, loss: 11.309473593150667, train acc: 0.96505, test acc: 0.9604\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normolize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 超参数\n",
    "iters_num = 20000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 平均每个epoch需要多少次迭代\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "print(f\"iter_per_epoch: {iter_per_epoch}\")\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 根据保存在grad中的梯度更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    \n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 计算每个epoch的准确率\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"epoch {i}, loss: {loss}, train acc: {train_acc}, test acc: {test_acc}\")\n",
    "\n",
    "import pickle\n",
    "# 保存训练好的网络对象\n",
    "with open('params.pkl', 'wb') as f:\n",
    "    pickle.dump(network, f)\n",
    "print('DONE')\n",
    "\n",
    "# 绘制图形\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-started",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
